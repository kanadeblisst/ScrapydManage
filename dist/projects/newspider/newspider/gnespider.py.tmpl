import time
import re
import hashlib

from scrapy.http import Request
from scrapy.spiders import Spider, Rule
from scrapy.linkextractors import LinkExtractor
from scrapy.loader import ItemLoader
from newspider.items import RuleSpiderItem
from newspider.extract_time import ExtractTime

def get_name_from_url(url):
    return re.search(r'\.(\w+)\.', url).group(1)

def get_domain_from_url(url):
    domain = re.search(r'://([\w.]+)/?', url).group(1)
    if domain.count('.') > 1:
        index = domain.index('.')
        domain = domain[index:]
    return domain

class GNESpider(Spider):
    url = 'http://www.example.com'
    name = get_name_from_url(url)
    allowed_domains = [get_domain_from_url(url)]
    time_extracter = ExtractTime()

    def start_requests(self):
        yield Request(self.url, callable=self.parse)

    def parse(self, response):
        new_urls = LinkExtractor(allow='.*').extract_links(response)
        for url in new_urls:
            yield response.follow(url, callable=self.parse)

        ir_mediasource = $ir_mediasource
        ir_mediatype = $ir_mediatype
        ir_librariytype = $ir_librariytype
        ir_area = $ir_area

        _url = response.url
        l = ItemLoader(item=RuleSpiderItem(), response=response)
        l.add_value('ir_mediasource', ir_mediasource)
        l.add_value('ir_mediatype', ir_mediatype)
        l.add_value('ir_librariytype', ir_librariytype)
        l.add_value('ir_trade', ir_trade)
        l.add_value('ir_area', ir_area) 
        l.add_value('ir_urldate', int(time.time()))
        l.add_value('ir_url', _url)
        extract_rule = $extract_rule
        for key in extract_rule:
            xpaths = extract_rule[key].get('xpath')
            if not xpaths:
                for xpath in xpaths:
                    l.add_xpath(key, xpath)
            re_list = extract_rule[key].get('re')
            if re_list:
                for re_ in re_list:
                    l.add_xpath(key, '/html', re=re_)
            replaces = extract_rule[key].get('replace')
            if not replaces:
                continue
            for i in replaces:
                new_value = l.get_collected_values(key)
                if isinstance(new_value, list):
                    for value in new_value:
                        if value:
                            l.replace_value(key, value.replace(i, ''))
                if isinstance(new_value, str):
                    l.replace_value(key, new_value.replace(i, ''))
                    
        if not l.get_collected_values('ir_content'):
            self.logger.warning(f'未提取到内容，请检查后重试! 当前URL：{_url}')
            return
        t = l.get_collected_values('ir_urltime')
        if not t:
            self.logger.warning(f'未提取到时间，请检查后重试! 当前URL：{_url}')
            return
        if not l.get_collected_values('ir_title'):
            ir_content = l.get_collected_values('ir_content')
            ir_title = ir_content[:60] if len(ir_content) > 60 else ir_content
            l.add_value('ir_title', ir_title)
        for i in t:
            timestamp = self.time_extracter.format_time(i)
            if timestamp:
                timestr = i
                break
        if not timestamp:
            self.logger.warning(f'时间解析失败!，时间字符串{t}')
            return
        self.logger.info('时间字符串：%s, 解析出的时间：%s'% (timestr, time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))))
        l.replace_value('ir_urltime', int(timestamp))
        l.add_value('ir_istrand', self.is_forward(ir_mediasource, l.get_collected_values('ir_firstauthor'))) 
        l.add_value('ir_md5', hashlib.md5(_url.encode('utf-8')).hexdigest())
        self.logger.info(l.load_item())
        yield l.load_item()
        
            
    def is_forward(self, ir_mediasource, ir_firstauthor):
        if not ir_firstauthor:
            return 1
        return 1 if set(ir_mediasource) & set(ir_firstauthor) else 2
